{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discussion_3_612.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB7Gh2mnJSb0",
        "colab_type": "text"
      },
      "source": [
        "# Research descussion assignment_3\n",
        "\n",
        "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.\n",
        "\n",
        "\n",
        "Recently, I watched a plenty of informnative webinars and talks talking maily at this topic. One of the most thought-provoking presentations at [TechSEO](https://www.catalystdigital.com/techseoboost/) Boost was the keynote by Dr. Ricardo Baeza-Yates, the CTO of NTENT. It was entitled, **“Biases in Search and Recommender Systems.”** \n",
        "\n",
        "\n",
        "## What is Bias?\n",
        "\n",
        "Dr. Baeza-Yates started by defining three different types of bias:\n",
        "Statistical: Significant systematic deviation from a prior (possibly unknown) distribution.\n",
        "\n",
        "* **Cultural**: Interpretations and judgments phenomena acquired through our life.\n",
        "* **Cognitive**: Systematic pattern of deviation from norm or rationality in judgment.\n",
        "\n",
        "Now, most critics of search and recommender systems focus on cultural biases, including: gender, racial, sexual, age, religious, social, linguistic, geographic, political, educational, economic, and technological.\n",
        "\n",
        "But, many people extrapolate results of a sample to the whole population – without considering statistical biases, including the gathering process, sampling process, validity, completeness, noise, or spam. In addition, there is cognitive bias when measuring bias. For example, one type of cognitive bias is confirmation bias, which is the tendency to search for, interpret, favor, and recall information in a way that affirms one’s prior beliefs or hypotheses. So, how does this impact search and recommender systems?\n",
        "\n",
        "Most web systems are optimized by using implicit user feedback. However, user data is partly biased by the choices that these systems make. For example, we can only click on things that are shown to us. Because these systems are usually based on Machine Learning, they learn to reinforce their own biases yielding self-fulfilled prophecies and/or sub-optimal solutions. For example, personalization and filter bubbles for users can create echo chambers for recommender systems. In addition, these systems sometimes compete among themselves. So, an improvement in one system (e.g., user experience) might be just a degradation in another system (e.g., monetization) that uses a different (even inversely correlated) optimization function.\n",
        "\n",
        "## What Is Being Fair?\n",
        "\n",
        "Dr. Baeza-Yates also tackled the question, **“What is being fair?”**\n",
        "\n",
        "He used images of three kids watching a soccer match to illustrate the difference between:\n",
        "\n",
        "* **Equality**, which assumes that everyone benefits from standing on boxes of the same height. This is the concept of equal treatment.\n",
        "\n",
        "* **Equity**, which argues that each kid should get the box that they need to see over the fence. This is the concept of “affirmative action.”\n",
        "\n",
        "* **Justice**, which enables all three kids to see the game without boxes because the cause of the inequity (the wooden fence) was addressed. This is the concept of removing the systemic barrier(s).\n",
        "\n",
        "So, the users of search and recommender systems need to realize that removing bias involves more than just making engineers tune their algorithms. It also requires users to be aware of their own cultural and cognitive biases.\n",
        "And this also means that search and recommender systems don’t need to be perfect, they just need to be better than humans who aren’t aware of their biases.\n",
        "\n",
        "\n",
        "## Biases Are Everywhere!\n",
        "Then, Dr. Baeza-Yates shared some research that found bias in places that most of us wouldn’t expect. These findings would have made great headlines, if he’d been interested in generating clickbait. But, a large part of the content of his presentation is available on his article, “Bias on the Web,” which was published in Communications of ACM in June 2018. And, in the context of his keynote presentation, they served as additional case studies that supported his analysis.\n",
        "\n",
        "For example, one study by Baeza-Yates, Castillo & López, which was published in Cybermetrics in 2005, found economic bias in links. (Specifically, it found that countries with more economic ties to Spain had more links to websites in Spain.)\n",
        "\n",
        "\n",
        "Another study, which was published on the Language Connect blog in 2012, found a language bias in web content. (Although about 27% of internet users speak English, 55.4% of the web content on the top 1 million websites is in English.)\n",
        "\n",
        "\n",
        "And a third study by Baeza-Yates & Saez Trumper, which was published in ACM Hypertext in 2015, found activity bias in user-generated content. (Forget the “wisdom of crowds.” Just 7% of Facebook users generated 50% of the posts in a small sample taken in 2008, 4% of Amazon users generated 50% of the movie reviews up to 2013, 2% of Twitter users generated 50% of the tweets in 2009, and only 0.04% of Wikipedia editors generated 50% of the English entries).\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "Dr. Baeza-Yates ended his keynote presentation with two key takeaways.\n",
        "\n",
        "The first was for the designers of search and recommender systems. They covered:\n",
        "1. **Data**\n",
        "\n",
        "  * Analyze for known and unknown biases, debias, or mitigate when possible/needed.\n",
        "  * Recollect more data for difficult/sparse regions of the problem.\n",
        "  * Delete attributes associated directly/indirectly with harmful bias.\n",
        "2. Interaction\n",
        "\n",
        "  * Make sure that the user is aware of the biases all the time.\n",
        "  * Give more control to the user.\n",
        "3. Design and Implementation\n",
        "\n",
        "  * Let experts/colleagues/users contest every step of the process.\n",
        "4. Evaluation\n",
        "\n",
        "  * Do not fool yourself!\n",
        "And for everyone in the audience at TechSEO Boost, he shared these messages:\n",
        "* Systems are a mirror of us – the good, the bad, and the ugly.\n",
        "* The Web amplifies everything, but always leaves traces.\n",
        "* We need to be aware of our own biases.\n",
        "* We must be aware of the biases and counteract them to stop the vicious bias cycle.\n",
        "* There are plenty of open (research) problems!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9TV500yMq_W",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "1. [what is ethical SEO](https://www.searchenginejournal.com/what-is-ethical-seo/318483/)\n",
        "2. [Tech seo boost](https://www.catalystdigital.com/techseoboost/)"
      ]
    }
  ]
}