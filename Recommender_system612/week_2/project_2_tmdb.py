# -*- coding: utf-8 -*-
"""project_2_movieLens.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NMr5qk25O8wGh21XCIv2tse739Bew1t_

## Simple Recommender system
"""

import pandas as pd 
import numpy as np

# Load Movies credit
df1 = pd.read_csv('/Users/salmaelshahawy/Desktop/138_4508_bundle_archive/tmdb_5000_credits.csv')

# Print the first three rows
df1.head()

# Load Movies 
df2 = pd.read_csv('/Users/salmaelshahawy/Desktop/138_4508_bundle_archive/tmdb_5000_movies.csv')

# Print the first three rows
df2.head()

# merge two tables on movie_id
df1.columns = ['id','movie_title','cast','crew']
df2 = df2.merge(df1, on='id')
df2.head()
# dataset.drop('title_y', axis=1).head()

"""# Demographic Filtering

I would use a weighted rating that takes into account the average rating and the number of votes it has accumulated. Such a system will make sure that a movie with a 9 rating from 100,000 voters gets a (far) higher score than a movie with the same rating but a mere few hundred voters.

WeightedRating(WR)= ((v/v+m)⋅R)+((m/v+m)⋅C)

* v is the number of votes for the movie;

* m is the minimum votes required to be listed in the chart;

* R is the average rating of the movie;

* C is the mean vote across the whole report.

I already have the values to v ```(vote_count)``` and R```(vote_average)``` for each movie in the dataset. It is also possible to directly calculate C from this data.
"""

# Calculate mean of vote average column
C = df2['vote_average'].mean()
print(C)

# Calculate the minimum number of votes required to be in the chart, m , coverage parameter
m = df2['vote_count'].quantile(0.90)
print(m)

# Filter out all qualified movies into a new DataFrame
q_movies = df2.copy().loc[df2['vote_count'] >= m]
q_movies.shape

q_movies.describe()

# Function that computes the weighted rating of each movie
def weighted_rating(x, m=m, C=C):
    v = x['vote_count']
    R = x['vote_average']
    # Calculation based on the IMDB formula
    return (v/(v+m) * R) + (m/(m+v) * C)

# Define a new feature 'score' and calculate its value with `weighted_rating()`
q_movies['score'] = q_movies.apply(weighted_rating, axis=1)

#Sort movies based on score calculated above
q_movies = q_movies.sort_values('score', ascending=False)

#Print the top 15 movies
q_movies[['title', 'vote_count', 'vote_average', 'score']].head()

import matplotlib.pyplot as plt

pop= df2.sort_values('popularity', ascending=False)
plt.figure(figsize=(12,4))

plt.barh(pop['title'].head(10),pop['popularity'].head(10), align='center',
        color='red')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Top 10 Popular Movies")

#Print overviews of the first 10 movies.
df2['overview'].head(10)

#Import TfIdfVectorizer from scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Replace NaN with an empty string
df2['overview'] = df2['overview'].fillna('')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix = tfidf.fit_transform(df2['overview'])

#Output the shape of tfidf_matrix
tfidf_matrix.shape

# Import linear_kernel
from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_sim

#Construct a reverse map of indices and movie titles
indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()
indices.head
indices['The Shawshank Redemption']

# Function that takes in movie title as input and outputs most similar movies
def get_recommendations(title, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1:11]

    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return df2['title'].iloc[movie_indices]

get_recommendations('The Shawshank Redemption')

"""# Collaborative filtering"""


from surprise import Reader
reader = Reader()
ratings = pd.read_csv('/Users/salmaelshahawy/Desktop/3405_6663_bundle_archive/ratings.csv')
ratings.head()

# from surprise import Dataset
# from surprise import SVD
# from surprise.model_selection import cross_validate
# data = Dataset.load_from_df(ratings[['userId', 'movieId','rating']], reader)
# svd = SVD()
# # Run 5-fold cross-validation and print results
# cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=3)

from surprise import Dataset
from surprise import SVD
from surprise.model_selection.split import KFold
data = Dataset.load_from_df(ratings[['userId', 'movieId','rating']], reader)
svd = SVD()
# Run 5-fold cross-validation and print results
kf = KFold(n_splits=3, random_state=42)
for trainset, testset in kf.split(data):

    # train and test algorithm.
    svd.fit(trainset)
    predictions = svd.test(testset)

    # Compute and print Root Mean Squared Error
    accuracy.rmse(predictions, verbose=True)