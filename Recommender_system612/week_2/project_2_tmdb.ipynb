{"cells":[{"metadata":{"id":"LZTPEOrbT4Uy"},"cell_type":"markdown","source":"**Author**: Salma Elshahawy\n\n**Date**: June, 5, 2020\n\n**Title**: DATA 612, Recommender system, project#1\n\n**Github repo**: [Simple_recommender#2](https://github.com/salma71/MSDS_SU2020/blob/master/Recommender_system612/week_2/project_2_tmdb.ipynb)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThe goal of this assignment is for you to try out different ways of implementing and configuring a recommender, and to evaluate your different approaches.\n\nIn this notebook, I will demonestrate two different methods for recommender system.\n\n* **Collaborative Filtering**: This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on a set of items, A is more likely to have B's opinion for a given item than that of a randomly chosen person.\n\n* **Content-Based Filtering**: This method uses only information about the description and attributes of the items users has previously consumed to model user's preferences. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended.\n"},{"metadata":{"id":"-_wMxxLqB6Ua","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"id":"bMEZqxbHKo2h","outputId":"b48db1fd-b120-430a-976b-b3bb4993d4dc","trusted":true},"cell_type":"code","source":"# Load Movies credit\ndf1 = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_credits.csv')\n\n# Print the first three rows\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Movie dataset contains the following features:-\n* **movie_id** - A unique identifier for each movie.\n* **cast** - The name of lead and supporting actors.\n* **crew** - The name of Director, Editor, Composer, Writer etc."},{"metadata":{"id":"KcyU8OBVEgE-","trusted":true},"cell_type":"code","source":"# Load Movies \ndf2 = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_movies.csv')\n\n# Print the first three rows\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The tmdb dataset has the following features:-\n\n* **budget** - The budget in which the movie was made.\n* **genre** - The genre of the movie, Action, Comedy ,Thriller etc.\n* **homepage** - A link to the homepage of the movie.\n* **id** - This is infact the movie_id as in the first dataset.\n* **keywords** - The keywords or tags related to the movie.\n* **original_language** - The language in which the movie was made.\n* **original_title** - The title of the movie before translation or adaptation.\n* **overview** - A brief description of the movie.\n* **popularity** - A numeric quantity specifying the movie popularity.\n* **production_companies** - The production house of the movie.\n* **production_countries** - The country in which it was produced.\n* **release_date** - The date on which it was released.\n* **revenue** - The worldwide revenue generated by the movie.\n* **runtime** - The running time of the movie in minutes.\n* **status** - \"Released\" or \"Rumored\".\n* **tagline** - Movie's tagline.\n* **title** - Title of the movie.\n* **vote_average** - average ratings the movie recieved.\n* ****vote_count** - the count of votes recieved."},{"metadata":{"id":"omdrwOvXExzJ","outputId":"2d34bf18-6660-4de3-ee2a-642484676915","trusted":true},"cell_type":"code","source":"# merge two tables on movie_id\ndf1.columns = ['id','movie_title','cast','crew']\ndf2 = df2.merge(df1, on='id')\ndf2.head()\n# dataset.drop('title_y', axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{"id":"azGPFYBsIVii"},"cell_type":"markdown","source":"# Explore the Data\n"},{"metadata":{"id":"j8jn1zZdSIjG"},"cell_type":"markdown","source":"I would use a weighted rating that takes into account the average rating and the number of votes it has accumulated. Such a system will make sure that a movie with a 9 rating from 100,000 voters gets a (far) higher score than a movie with the same rating but a mere few hundred voters."},{"metadata":{"id":"0Hlsh_GvSVvK"},"cell_type":"markdown","source":"WeightedRating(WR)= ((v/v+m)⋅R)+((m/v+m)⋅C)\n\n* v is the number of votes for the movie;\n\n* m is the minimum votes required to be listed in the chart;\n\n* R is the average rating of the movie;\n\n* C is the mean vote across the whole report."},{"metadata":{"id":"uAVeoDM2SyQV"},"cell_type":"markdown","source":"I already have the values to v ```(vote_count)``` and R```(vote_average)``` for each movie in the dataset. It is also possible to directly calculate C from this data."},{"metadata":{"id":"e32fF40ESQgP","outputId":"bdeb7207-8854-4e4a-f536-bd7693eee760","trusted":true},"cell_type":"code","source":"# Calculate mean of vote average column\nC = df2['vote_average'].mean()\nprint(C)","execution_count":null,"outputs":[]},{"metadata":{"id":"3Yjsal_fTClC","outputId":"b2241455-6926-426a-fa20-30e70c288a54","trusted":true},"cell_type":"code","source":"# Calculate the minimum number of votes required to be in the chart, m , coverage parameter\nm = df2['vote_count'].quantile(0.90)\nprint(m)","execution_count":null,"outputs":[]},{"metadata":{"id":"47HzFyFUTJ4I","outputId":"b1c4f5d1-4eb9-4730-c0b4-ad7afca09c85","trusted":true},"cell_type":"code","source":"# Filter out all qualified movies into a new DataFrame\nq_movies = df2.copy().loc[df2['vote_count'] >= m]\nq_movies.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"pkrGxTCxTU4f","outputId":"dc6eaad8-2081-4626-983d-23b3e555169a","trusted":true},"cell_type":"code","source":"q_movies.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"PL1taN2cTmBW","trusted":true},"cell_type":"code","source":"# Function that computes the weighted rating of each movie\ndef weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # Calculation based on the IMDB formula\n    return (v/(v+m) * R) + (m/(m+v) * C)","execution_count":null,"outputs":[]},{"metadata":{"id":"oszrdSK7TrpM","trusted":true},"cell_type":"code","source":"# Define a new feature 'score' and calculate its value with `weighted_rating()`\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"hFZ0ggLoTvQf","outputId":"96e74e43-d8e3-4477-a85f-96306f72d867","trusted":true},"cell_type":"code","source":"#Sort movies based on score calculated above\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#Print the top 15 movies\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"MjboiPGnKKQz","outputId":"cc767d87-5c71-443b-b5ea-6c1ffe6e0034","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\npop= df2.sort_values('popularity', ascending=False)\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(10),pop['popularity'].head(10), align='center',\n        color='red')\nplt.gca().invert_yaxis()\nplt.xlabel(\"Popularity\")\nplt.title(\"Top 10 Popular Movies\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the model"},{"metadata":{},"cell_type":"markdown","source":"## Content based filtering"},{"metadata":{},"cell_type":"markdown","source":"Content-based filtering approaches leverage description or attributes from items the user has interacted to recommend similar items. It depends only on the user previous choices, making this method robust to avoid the cold-start problem. For textual items, like articles, news and books, it is simple to use the raw text to build item profiles and user profiles. Here we are using a very popular technique in information retrieval (search engines) named TF-IDF. This technique converts unstructured text into a vector structure, where each word is represented by a position in the vector, and the value measures how relevant a given word is for an article. As all items will be represented in the same Vector Space Model, it is to compute similarity between movie overview."},{"metadata":{"id":"57XCJrZOVZVN","outputId":"e4c66fb0-dddb-4d68-ba6a-874ed46a90e7","trusted":true},"cell_type":"code","source":"#Print overviews of the first 10 movies.\ndf2['overview'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"BIEgqRmbWHSz","outputId":"235f7c6b-4f63-4163-e333-06bca652f55d","trusted":true},"cell_type":"code","source":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\ndf2['overview'] = df2['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"JFILEqbDWwFc","trusted":true},"cell_type":"code","source":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"id":"RQO3uu7yW0Pi","outputId":"40c81652-f3ec-4085-eb4d-e4afedb39c9c","trusted":true},"cell_type":"code","source":"cosine_sim","execution_count":null,"outputs":[]},{"metadata":{"id":"t4_5k13_L4vA","outputId":"f5c4df6b-a20d-4d5f-b1ab-c794467c88e5","trusted":true},"cell_type":"code","source":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(df2.index, index=df2['title']).drop_duplicates()\nindices.head\nindices['The Shawshank Redemption']","execution_count":null,"outputs":[]},{"metadata":{"id":"OdsXY-EDMNuR","trusted":true},"cell_type":"code","source":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return df2['title'].iloc[movie_indices]","execution_count":null,"outputs":[]},{"metadata":{"id":"YGXPPIQ_Mo76","outputId":"64e94fab-cc5e-41cc-eecd-efddd4ebb102","trusted":true},"cell_type":"code","source":"get_recommendations('Batman Forever')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collaborative filtering"},{"metadata":{},"cell_type":"markdown","source":"Collaborative Filtering (CF) has two main implementation strategies:\n\n1. **Memory-based**: This approach uses the memory of previous users interactions to compute users similarities based on items they've interacted (user-based approach) or compute items similarities based on the users that have interacted with them (item-based approach). A typical example of this approach is User Neighbourhood-based CF, in which the top-N similar users (usually computed using Pearson correlation) for a user are selected and used to recommend items those similar users liked, but the current user have not interacted yet. This approach is very simple to implement, but usually do not scale well for many users. \n\n2. **Model-based**: This approach, models are developed using different machine learning algorithms to recommend items to users. There are many model-based CF algorithms, like neural networks, bayesian networks, clustering models, and latent factor models such as Singular Value Decomposition (SVD) and, probabilistic latent semantic analysis."},{"metadata":{"id":"772sZm8xMs0d","trusted":true},"cell_type":"code","source":"from surprise import Reader, Dataset, SVD\nreader = Reader()\nratings = pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import cross_validate\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n\n# use the famous SVD algorithm.\nalgo = SVD()\n\n# Run 5-fold cross-validation and print results\ncross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got ana average of 0.89 of RMSE, which is good in our case"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data.build_full_trainset()\nalgo.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings[ratings['userId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's apply the predict method to see the performance of the recommender. the predict method takes three arguments, user_id, movie_id, and the true rating. The output is the prediction with the estimated new one. "},{"metadata":{"trusted":true},"cell_type":"code","source":"algo.predict(1, 2150, 3.0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}