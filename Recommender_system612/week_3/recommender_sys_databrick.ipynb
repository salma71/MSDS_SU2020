{"cells":[{"cell_type":"markdown","source":["**Author**: Salma Elshahawy\n\n**Date**: June, 18, 2020\n\n**Title**: DATA 612, Recommender system, project#3\n\n**Github repo**: [Matrix factorization using ALS project#3](https://github.com/salma71/MSDS_SU2020/blob/master/Recommender_system612/week_3/recommender_sys_databrick.ipynb)"],"metadata":{}},{"cell_type":"markdown","source":["## Introduction\n\nFor this week assignment, I used databricks notebook to build the recommender system, I used the movielens dataset with 20m observation"],"metadata":{}},{"cell_type":"markdown","source":["## Setting up databrick environment"],"metadata":{}},{"cell_type":"code","source":["# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["import os\n# from test_helper import Test\n\n\ndbfs_dir = '/databricks-datasets/cs110x/ml-20m/data-001'\nratings_filename = dbfs_dir + '/ratings.csv'\nmovies_filename = dbfs_dir + '/movies.csv'\n\n# The following line is here to enable this notebook to be exported as source and\n# run on a local machine with a local copy of the files. Just change the dbfs_dir,\n# above.\nif os.path.sep != '/':\n  # Handle Windows.\n  ratings_filename = ratings_filename.replace('/', os.path.sep)\n  movie_filename = movie_filename.replace('/', os.path.sep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["display(dbutils.fs.ls(dbfs_dir))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/README.txt</td><td>README.txt</td><td>8964</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/links.csv</td><td>links.csv</td><td>569517</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/links.csv.gz</td><td>links.csv.gz</td><td>245973</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/movies.csv</td><td>movies.csv</td><td>1397542</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/movies.csv.gz</td><td>movies.csv.gz</td><td>498839</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/ratings.csv</td><td>ratings.csv</td><td>533444411</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/ratings.csv.gz</td><td>ratings.csv.gz</td><td>132656084</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/tags.csv</td><td>tags.csv</td><td>16603996</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/ml-20m/data-001/tags.csv.gz</td><td>tags.csv.gz</td><td>4787917</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# deal with the comprissed files ending in (.gz)\nfrom pyspark.sql.types import *\n\nratings_df_schema = StructType(\n  [StructField('userId', IntegerType()),\n   StructField('movieId', IntegerType()),\n   StructField('rating', DoubleType())]\n)\nmovies_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType())]\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# caching the data on top of S3 in memory \nfrom pyspark.sql.functions import regexp_extract\nfrom pyspark.sql.types import *\n\nraw_ratings_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\nratings_df = raw_ratings_df.drop('Timestamp')\n\nraw_movies_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_df_schema).load(movies_filename)\nmovies_df = raw_movies_df.drop('Genres').withColumnRenamed('movieId', 'ID')\n\nratings_df.cache()\nmovies_df.cache()\n\nassert ratings_df.is_cached\nassert movies_df.is_cached\n\nraw_ratings_count = raw_ratings_df.count()\nratings_count = ratings_df.count()\nraw_movies_count = raw_movies_df.count()\nmovies_count = movies_df.count()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Reading the dataset"],"metadata":{}},{"cell_type":"code","source":["print ('There are %s ratings and %s movies in the datasets' % (ratings_count, movies_count))\nprint ('Ratings:')\nratings_df.show(3)\nprint ('Movies:')\ndisplay(movies_df.head(10))\n\nassert raw_ratings_count == ratings_count\nassert raw_movies_count == movies_count"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>title</th></tr></thead><tbody><tr><td>1</td><td>Toy Story (1995)</td></tr><tr><td>2</td><td>Jumanji (1995)</td></tr><tr><td>3</td><td>Grumpier Old Men (1995)</td></tr><tr><td>4</td><td>Waiting to Exhale (1995)</td></tr><tr><td>5</td><td>Father of the Bride Part II (1995)</td></tr><tr><td>6</td><td>Heat (1995)</td></tr><tr><td>7</td><td>Sabrina (1995)</td></tr><tr><td>8</td><td>Tom and Huck (1995)</td></tr><tr><td>9</td><td>Sudden Death (1995)</td></tr><tr><td>10</td><td>GoldenEye (1995)</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# quick verification on data\nassert ratings_count == 20000263\nassert movies_count == 27278\nassert movies_df.filter(movies_df.title == 'Toy Story (1995)').count() == 1\nassert ratings_df.filter((ratings_df.userId == 6) & (ratings_df.movieId == 1) & (ratings_df.rating == 5.0)).count() == 1\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["display(movies_df.head(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>title</th></tr></thead><tbody><tr><td>1</td><td>Toy Story (1995)</td></tr><tr><td>2</td><td>Jumanji (1995)</td></tr><tr><td>3</td><td>Grumpier Old Men (1995)</td></tr><tr><td>4</td><td>Waiting to Exhale (1995)</td></tr><tr><td>5</td><td>Father of the Bride Part II (1995)</td></tr><tr><td>6</td><td>Heat (1995)</td></tr><tr><td>7</td><td>Sabrina (1995)</td></tr><tr><td>8</td><td>Tom and Huck (1995)</td></tr><tr><td>9</td><td>Sudden Death (1995)</td></tr><tr><td>10</td><td>GoldenEye (1995)</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["display(ratings_df.head(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>userId</th><th>movieId</th><th>rating</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>3.5</td></tr><tr><td>1</td><td>29</td><td>3.5</td></tr><tr><td>1</td><td>32</td><td>3.5</td></tr><tr><td>1</td><td>47</td><td>3.5</td></tr><tr><td>1</td><td>50</td><td>3.5</td></tr><tr><td>1</td><td>112</td><td>3.5</td></tr><tr><td>1</td><td>151</td><td>4.0</td></tr><tr><td>1</td><td>223</td><td>4.0</td></tr><tr><td>1</td><td>253</td><td>4.0</td></tr><tr><td>1</td><td>260</td><td>4.0</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["One way to recommend movies is to find the movies with the highest average ratings.\n\nI will use spark to find the name, number of ratings, and average ratings for the top 20 movies with at least 500 reviews.\n\n* get ratings_df into a new dataframe called movie_ids_with_avg_ratings that have:\n  1. movieId, \n  2. no_of ratings per movie, \n  3. avg rating for all movies\n\n* add movie title column to the movie_ids_with_avg_ratings into a new dataframe called movie_names_with_avg_ratings_df\n* will do join for the last step"],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\n\nmovie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias('count'), F.avg(ratings_df.rating).alias('average'))\n\nprint('movie_ids_with_avg_ratings_df:')\n\ndisplay(movie_ids_with_avg_ratings_df.head(10))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>count</th><th>average</th></tr></thead><tbody><tr><td>3997</td><td>2047</td><td>2.0703468490473864</td></tr><tr><td>1580</td><td>35580</td><td>3.55831928049466</td></tr><tr><td>3918</td><td>1246</td><td>2.918940609951846</td></tr><tr><td>2366</td><td>6627</td><td>3.5492681454655197</td></tr><tr><td>3175</td><td>13945</td><td>3.600717102904267</td></tr><tr><td>4519</td><td>1936</td><td>3.2463842975206614</td></tr><tr><td>1591</td><td>5255</td><td>2.6201712654614653</td></tr><tr><td>471</td><td>11268</td><td>3.6641817536386228</td></tr><tr><td>36525</td><td>1169</td><td>3.482891360136869</td></tr><tr><td>44022</td><td>2465</td><td>3.334077079107505</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["movie_names_df = movies_df.withColumnRenamed('ID', 'movieId')\nmovie_names_with_avg_ratings_df = movie_ids_with_avg_ratings_df.join(movie_names_df, on=['movieId'], how='inner')\nprint('movie_names_with_avg_ratings_df:')\ndisplay(movie_names_with_avg_ratings_df.head(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>count</th><th>average</th><th>title</th></tr></thead><tbody><tr><td>3997</td><td>2047</td><td>2.0703468490473864</td><td>Dungeons & Dragons (2000)</td></tr><tr><td>1580</td><td>35580</td><td>3.55831928049466</td><td>Men in Black (a.k.a. MIB) (1997)</td></tr><tr><td>3918</td><td>1246</td><td>2.918940609951846</td><td>Hellbound: Hellraiser II (1988)</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"code","source":["#Now that we have a DataFrame of the movies with highest average ratings, we can use Spark to determine the 20 movies with highest average ratings and at least 500 reviews. \n\nmovies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter(F.col('count')>=500)\\\n  .sort('average')\n\nprint ('Movies with highest ratings:')\ndisplay(movies_with_500_ratings_or_more.head(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>count</th><th>average</th><th>title</th></tr></thead><tbody><tr><td>4775</td><td>685</td><td>1.1240875912408759</td><td>Glitter (2001)</td></tr><tr><td>6587</td><td>701</td><td>1.1640513552068474</td><td>Gigli (2003)</td></tr><tr><td>1495</td><td>715</td><td>1.3902097902097903</td><td>Turbo: A Power Rangers Movie (1997)</td></tr><tr><td>50798</td><td>899</td><td>1.4888765294771968</td><td>Epic Movie (2007)</td></tr><tr><td>1739</td><td>623</td><td>1.5457463884430176</td><td>3 Ninjas: High Noon On Mega Mountain (1998)</td></tr><tr><td>4241</td><td>571</td><td>1.5726795096322241</td><td>Pok√©mon 3: The Movie (2001)</td></tr><tr><td>3593</td><td>3973</td><td>1.6005537377296752</td><td>Battlefield Earth (2000)</td></tr><tr><td>2386</td><td>591</td><td>1.619289340101523</td><td>Jerry Springer: Ringmaster (1998)</td></tr><tr><td>57532</td><td>615</td><td>1.6276422764227643</td><td>Meet the Spartans (2008)</td></tr><tr><td>1599</td><td>519</td><td>1.6666666666666667</td><td>Steel (1997)</td></tr><tr><td>43919</td><td>769</td><td>1.6671001300390118</td><td>Date Movie (2006)</td></tr><tr><td>2817</td><td>726</td><td>1.677685950413223</td><td>Aces: Iron Eagle III (1992)</td></tr><tr><td>2555</td><td>1399</td><td>1.7030021443888492</td><td>Baby Geniuses (1999)</td></tr><tr><td>1595</td><td>941</td><td>1.706163655685441</td><td>Free Willy 3: The Rescue (1997)</td></tr><tr><td>2462</td><td>554</td><td>1.7193140794223827</td><td>Texas Chainsaw Massacre: The Next Generation (a.k.a. The Return of the Texas Chainsaw Massacre) (1994)</td></tr><tr><td>8387</td><td>927</td><td>1.7481121898597627</td><td>Police Academy: Mission to Moscow (1994)</td></tr><tr><td>2799</td><td>1125</td><td>1.76</td><td>Problem Child 2 (1991)</td></tr><tr><td>1981</td><td>1192</td><td>1.7600671140939597</td><td>Friday the 13th Part VIII: Jason Takes Manhattan (1989)</td></tr><tr><td>3268</td><td>1835</td><td>1.768392370572207</td><td>Stop! Or My Mom Will Shoot (1992)</td></tr><tr><td>1760</td><td>2658</td><td>1.770316027088036</td><td>Spice World (1997)</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["## Creating a Training Set\nbreak the ratings_df into three parts:\n\n* A training set (DataFrame), which we will use to train models\n* A validation set (DataFrame), which we will use to choose the best model\n* A test set (DataFrame), which we will use for our experiments\n* To randomly split the dataset into the multiple groups, we can use the pySpark randomSplit() transformation. randomSplit() takes a set of splits and a seed and returns multiple DataFrames."],"metadata":{}},{"cell_type":"code","source":["# hold out 60% for training, 20% of our data for validation, and leave 20% for testing\n\nseed = 42\n(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed = seed)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# cache the resulted datasets\ntraining_df = split_60_df.cache()\nvalidation_df = split_a_20_df.cache()\ntest_df = split_b_20_df.cache()\n\nprint('Training: {0}, validation: {1}, test: {2}\\n'.format(\n  training_df.count(), validation_df.count(), test_df.count())\n)\ndisplay(training_df.head(3))\ndisplay(validation_df.head(3))\ndisplay(test_df.head(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>userId</th><th>movieId</th><th>rating</th></tr></thead><tbody><tr><td>1</td><td>29</td><td>3.5</td></tr><tr><td>1</td><td>32</td><td>3.5</td></tr><tr><td>1</td><td>47</td><td>3.5</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["After splitting the dataset, the training set has about 12 million entries and the validation and test sets each have about 4 million entries. (The exact number of entries in each dataset varies slightly due to the random nature of the randomSplit() transformation.)\n\n## Alternating Least Squares\nIn this part, I will use the Apache Spark ML Pipeline implementation of Alternating Least Squares, ALS. \n\nALS takes a training dataset (DataFrame) and several parameters that control the model creation process. To determine the best values for the parameters, I will use ALS to train several models, and then I will select the best model and use the parameters from that model in the rest.\n\nThe process to for determining the best model is as follows:\n\n1. Pick a set of model parameters. The most important parameter to model is the **rank**, which is the number of columns in the Users matrix or the number of rows in the Movies matrix. A lower ran means higher error on the training dataset, but a high rank may lead to overfitting. I will train models with ranks of 4, 8, and 12 using the ```training_df``` dataset.\n\n2. Set the appropriate parameters on the ALS object:\n\n* The \"User\" column will be set to the values in the userId DataFrame column.\n* The \"Item\" column will be set to the values in the movieId DataFrame column.\n* The \"Rating\" column will be set to the values in the rating DataFrame column.\n* I'll using a regularization parameter of 0.1.\n\n\n3. Create multiple models using ALS.fit(), one for each of the rank values. I'll fit against the training data set ```(training_df)```.\n\nFor each model, I'll run a prediction against the validation data set (validation_df) and check the error.\n\nI'll keep the model with the best error rate."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\nals = ALS(maxIter = 5, regParam = 0.1, userCol = 'userId', itemCol = 'movieId', ratingCol='rating', coldStartStrategy = 'drop')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n# Create an RMSE evaluator using the label and predicted columns\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n\ntolerance = 0.03\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nmodels = [0, 0, 0]\nerr = 0\nmin_error = float('inf')\nbest_rank = -1\n\nfor rank in ranks:\n  # Set the rank here:\n  als.setRank(5)\n  # Create the model with these parameters.\n  model = als.fit(training_df)\n  # Run the model to create a prediction. Predict against the validation_df.\n  predict_df = model.transform(validation_df)\n\n  # Remove NaN values from prediction (due to SPARK-14489)\n  predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n  # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n  error = reg_eval.evaluate(predict_df)\n  errors[err] = error\n  models[err] = model\n  print ('For rank %s the RMSE is %s' % (rank, error))\n  if error < min_error:\n    min_error = error\n    best_rank = err\n  err += 1\n\nals.setRank(ranks[best_rank])\nprint ('The best model was trained with rank %s' % ranks[best_rank])\nmy_model = models[best_rank]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">For rank 4 the RMSE is 0.8222388447502935\nFor rank 8 the RMSE is 0.8222388447502935\nFor rank 12 the RMSE is 0.8222388447502935\nThe best model was trained with rank 4\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Testing the Model\nSo far, I used the ```training_df``` and ```validation_df``` datasets to select the best model. Since I used these two datasets to determine what model is best, I cannot use them to test how good the model is; otherwise, I would be very vulnerable to **overfitting**. To decide how good the model is, I need to use the ```test_df``` dataset. I will use the ```best_rank``` I determined before to create a model for predicting the ratings for the test dataset and then we will compute the ```RMSE```.\n\nThe steps you should perform are:\n\n* Run a prediction, using my_model as created above, on the test dataset (test_df), producing a new predict_df DataFrame.\n* Filter out unwanted NaN values. We've supplied this piece of code for you.\n* Use the previously created RMSE evaluator, reg_eval to evaluate the filtered DataFrame."],"metadata":{}},{"cell_type":"code","source":["# In ML Pipelines, this next step has a bug that produces unwanted NaN values. We\n# have to filter them out. See https://issues.apache.org/jira/browse/SPARK-14489\npredict_df = my_model.transform(test_df)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The model had a RMSE on the test set of 0.8226318256461504\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["## Comparing the Model\nLooking at the RMSE for the results predicted by the model versus the values in the test set is one way to evalute the quality of the model. Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set.\n\n### The steps are:\n\n1. Use the training_df to compute the average rating across all movies in that training dataset.\n2. Use the average rating that you just determined and the test_df to create a DataFrame (test_for_avg_df) with a prediction column containing the average rating. \n3. Use the previously created reg_eval object to evaluate the test_for_avg_df and calculate the RMSE."],"metadata":{}},{"cell_type":"code","source":["avg_rating_df = training_df.select(F.avg(training_df.rating))\navg_rating_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+\n       avg(rating)|\n+------------------+\n3.5257348832825692|\n+------------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["# Compute the average rating\n\n# Extract the average rating value. (This is row 0, column 0.)\ntraining_avg_rating = avg_rating_df.collect()[0][0]\ntraining_avg_rating\nprint('The average rating for movies in the training set is {0}'.format(training_avg_rating))\n\n# Add a column with the average rating\n# df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n# [Row(height=5, spark_user=True)]\n# test_for_avg_df = test_df.select(lit(training_avg_rating).alias('average')).withColumn('prediction', lit(True)).take(1)\ntest_for_avg_df = test_df.withColumn('prediction', F.lit(training_avg_rating))\n\n# Run the previously created RMSE evaluator, reg_eval, on the test_for_avg_df DataFrame\ntest_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n\nprint(\"The RMSE on the average set is {0}\".format(test_avg_RMSE))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The average rating for movies in the training set is 3.5257348832825692\nThe RMSE on the average set is 1.0521941515201385\n</div>"]}}],"execution_count":28}],"metadata":{"name":"recommender system","notebookId":2032159490013599},"nbformat":4,"nbformat_minor":0}
